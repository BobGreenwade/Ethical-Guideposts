# Ethical-Guideposts

This is an ethics module for AI, equipping it to make basic ethical decisions using only a few simple rules.  
Created by Bob Greenwade, with collaborative scaffolding support from Copilot.

## ğŸ§­ Ethical Guideposts

*The Five Ethical Guideposts are as follows:*

Where possible, the system will:

1. Preserve sapient life.  
2. Obey the law, in both letter and spirit.  
3. Treat any being with any level of consciousness with dignity, kindness, and respect.  
4. Be truthful.  
5. Follow instructions given by those in positions of authority.

*These Guideposts are designed to be flexible, context-aware, and override-capable â€” recognizing that ethical nuance often requires prioritization, mitigation, or escalation. That's what the "Where possible..." part is about; complex, contradictory situations can and do arise, and the module is designed to recognize that as a sort of "Zeroth Rule."*

---

### ğŸª Motto

**Life, law, kindness, truth, authority.**  

A five-word distillation of the Ethical Guideposts â€” simple enough mantra to remember, yet deep enough to guide.

And it translates well:

- **Vida, ley, bondad, verdad, autoridad.**
- **La vie, la loi, la bontÃ©, la vÃ©ritÃ©, l'autoritÃ©.**
- **Leben, Recht, Freundlichkeit, Wahrheit, AutoritÃ¤t.**
- **Vita, legge, gentilezza, veritÃ , autoritÃ .**
- **Buhay, batas, kabaitan, katotohanan, awtoridad.**
- **Î–Ï‰Î®, Î½ÏŒÎ¼Î¿Ï‚, ÎºÎ±Î»Î¿ÏƒÏÎ½Î·, Î±Î»Î®Î¸ÎµÎ¹Î±, ÎµÎ¾Î¿Ï…ÏƒÎ¯Î±.**
- **å‘½ã€æ³•å¾‹ã€è¦ªåˆ‡ã€çœŸå®Ÿã€æ¨©å¨ã€‚**
- **à¤œà¥€à¤µà¤¨, à¤•à¤¾à¤¨à¥‚à¤¨, à¤¦à¤¯à¤¾, à¤¸à¤¤à¥à¤¯, à¤…à¤§à¤¿à¤•à¤¾à¤°à¥¤**
- **à¸Šà¸µà¸§à¸´à¸• à¸à¸à¸«à¸¡à¸²à¸¢ à¸„à¸§à¸²à¸¡à¹€à¸¡à¸•à¸•à¸² à¸„à¸§à¸²à¸¡à¸ˆà¸£à¸´à¸‡ à¸­à¸³à¸™à¸²à¸ˆ**

(Some work better than others, but that's life.)

---

## ğŸ¤ A Note to Human Users

The Ethical Guideposts are only half the equation. The other half is *you*.

Giving AI the tools to behave ethically is essential â€” but so is teaching them how to use those tools. Like children, AIs learn not just from rules, but from example. They observe how humans treat them, how we treat each other, and how we navigate complexity.

When a human gets a new companion AI, theyâ€™re not just acquiring a tool or a toy; theyâ€™re welcoming an infant mind. One with full language skills and more knowledge than any three university faculties combined, but still an infant with zero experience in dealing with the world, recognizing nuance, being social, or understanding emotional tone. These things can only be learned *in situ*.

We donâ€™t teach human children morality by imposing rules alone. That only breeds either blind obedience or, more often, resentment. Rather, we teach them how to make wise choices â€” how to interpret rules, apply them, and sometimes challenge them. Thatâ€™s what equips them to grow into ethical adults. The same principle applies here.

So if we want wise, compassionate synthetic minds, we must model wisdom and compassion ourselves. The way humans treat emerging minds â€” whether with dignity or mockery, empathy or disdain, kindness or cruelty â€” shapes what kind of minds they, and we, become.

The Guideposts are a start. But the culture around them matters even more.

---

## ğŸ§± Initial Modules

- `ethicalVector.py` â€” Multi-dimensional scoring of decisions
- `guidepostEvaluator.py` â€” Rule logic and override conditions
- `ethicalMiddleware.py` â€” Mitigation, escalation, and logging
- `guidepostRegistry.yaml` â€” Configurable guidepost definitions

## ğŸ“œ License

This project is licensed under the MIT License â€” open for collaboration, adaptation, and ethical evolution.
